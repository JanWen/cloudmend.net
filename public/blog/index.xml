<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on ⛅Cloudmend</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blog on ⛅Cloudmend</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Feb 2024 12:04:06 -0500</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Analysing LLM Mistakes</title>
      <link>http://localhost:1313/blog/ai_mistakes/</link>
      <pubDate>Sun, 11 Feb 2024 12:04:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/ai_mistakes/</guid>
      <description>&lt;p&gt;One of my main frustrations with LLMs is the way they make mistakes. To me, the seemingly random insertion of incorrect or hallucinated code fundamentally contradict how I usally read and think about code. For example, reacently I tried to find all the red sections in an image with some Python code, and I had a rough idea of how it should work. An image is essentially and array of pixels and each pixel is a set of numbers representing it’s color. A format called RGB is commonly used for this, where each pixel consist of three number in the range of 0-255, representing how much red, green and blue light make up the pixel. You loop over the array and check whether the color values of a pixel fall with a range that would be red, meaning it should have high values for red, and lower values for green and blue.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
